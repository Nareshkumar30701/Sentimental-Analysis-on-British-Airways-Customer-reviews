# -*- coding: utf-8 -*-
"""BA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qaFkIrLiCHy9Q2zGrTSALPjP1lF0TlIR
"""

from bs4 import BeautifulSoup
import requests

# Function to retrieve reviews from a single page
def get_reviews_from_page(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        review_divs = soup.find_all('div', class_='text_content')
        reviews = [review_div.get_text(strip=True) for review_div in review_divs]
        return reviews
    else:
        print("Failed to retrieve data from the website. Status code:", response.status_code)
        return []

# Main function to retrieve at least 100 reviews
def get_100_reviews():
    base_url = 'https://www.airlinequality.com/airline-reviews/british-airways'
    reviews = []
    page_number = 1
    while len(reviews) < 100:
        url = f"{base_url}/page/{page_number}/"
        page_reviews = get_reviews_from_page(url)
        if not page_reviews:
            break
        reviews.extend(page_reviews)
        page_number += 1
    return reviews

# Retrieve at least 100 reviews
reviews = get_100_reviews()
print("Total number of reviews retrieved:", len(reviews))

import pandas as pd
dff=pd.DataFrame()

dff['reviews']=reviews

dff

dff.shape

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
# Function to clean text
def clean_text(text):
    # Remove special characters, symbols, and emojis
    text_cleaned = re.sub(r'[^\w\s]', '', text)

    # Convert to lowercase
    text_cleaned = text_cleaned.lower()

    # Tokenization
    tokens = word_tokenize(text_cleaned)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Join tokens back into a string
    text_cleaned = ' '.join(tokens)

    return text_cleaned

# Apply the clean_text function to the 'reviews' column
dff['cleaned_reviews'] = dff['reviews'].apply(clean_text)

dff

# Remove the common phrase "Trip Verified" from all reviews in the DataFrame
dff['cleaned_reviews'] = dff['cleaned_reviews'].str.replace("trip verified", "").str.strip()

# Print the updated DataFrame
print(dff['cleaned_reviews'])

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Initialize CountVectorizer to convert text data to a document-term matrix
vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')

# Fit and transform the cleaned reviews to a document-term matrix
dtm = vectorizer.fit_transform(dff['cleaned_reviews'])

# Initialize LDA model
num_topics = 3  # Adjust the number of topics as needed
lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)

# Fit the LDA model to the document-term matrix
lda_model.fit(dtm)

# Print the top words for each topic
feature_names = vectorizer.get_feature_names_out()
for topic_idx, topic in enumerate(lda_model.components_):
    print(f"Topic {topic_idx + 1}:")
    top_words_idx = topic.argsort()[:-11:-1]
    top_words = [feature_names[i] for i in top_words_idx]
    print(", ".join(top_words))
    print()

# Get the document-topic matrix from the LDA model
doc_topic_matrix = lda_model.transform(dtm)

# Iterate over each topic
for topic_idx, topic in enumerate(lda_model.components_):
    print(f"Reviews in Topic {topic_idx + 1}:")

    # Get the indices of reviews associated with the current topic
    topic_reviews_indices = doc_topic_matrix[:, topic_idx].argsort()[::-1]

    # Print the top reviews for the current topic
    for idx in topic_reviews_indices[:5]:
        print(dff['cleaned_reviews'].iloc[idx])
        print()

# Compute perplexity using the score method
perplexity = lda_model.score(dtm)
print("Perplexity:", perplexity)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Function to generate and plot word cloud for a given topic
def plot_word_cloud(topic_words, topic_num):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(topic_words))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'Topic {topic_num} Word Cloud')
    plt.axis('off')
    plt.show()

# Print and plot word clouds for each topic
for topic_idx, topic in enumerate(lda_model.components_):
    top_words_idx = topic.argsort()
    top_words = [feature_names[i] for i in top_words_idx]

    print(f"Topic {topic_idx + 1} Top Words:", ', '.join(top_words))

    plot_word_cloud(top_words, topic_idx + 1)

from textblob import TextBlob
# Modified sentiment analysis function to return 'Neutral' if the polarity is close to zero
def analyze_sentiment(text):
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity

    if polarity > 0:
        return "Positive"
    else:
        return "Negative"



# Apply sentiment analysis to each review and create a new column with sentiment labels
dff['sentiment'] = dff['cleaned_reviews'].apply(analyze_sentiment)

# Print the DataFrame with sentiment labels
print(dff[['cleaned_reviews', 'sentiment']])

dff

import matplotlib.pyplot as plt

# Count the number of positive and negative sentiments
sentiment_counts = dff['sentiment'].value_counts()

# Plot the bar graph
plt.figure(figsize=(6, 4))
sentiment_counts.plot(kind='bar', color=['lightblue', 'blue'])
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

# Initialize dictionaries to store sentiment counts for each topic
topic_sentiment_counts = {topic_idx: {'Positive': 0, 'Negative': 0} for topic_idx in range(num_topics)}

# Perform sentiment analysis and count sentiment occurrences within each topic
for idx, review in enumerate(dff['cleaned_reviews']):
    # Perform sentiment analysis on the review
    sentiment = analyze_sentiment(review)

    # Determine the topic of the review
    topic_idx = lda_model.transform(dtm[idx:idx+1]).argmax()

    # Increment sentiment count for the corresponding topic
    topic_sentiment_counts[topic_idx][sentiment] += 1

# Print the sentiment counts for each topic
for topic_idx, sentiment_counts in topic_sentiment_counts.items():
    print(f"Topic {topic_idx + 1} Sentiment Counts:", sentiment_counts)

positive_counts = [sentiment_counts['Positive'] for sentiment_counts in topic_sentiment_counts.values()]
negative_counts = [sentiment_counts['Negative'] for sentiment_counts in topic_sentiment_counts.values()]

# Plot the bar graph
plt.figure(figsize=(10, 6))
plt.bar(range(1, num_topics + 1), positive_counts, color='lightblue', label='Positive')
plt.bar(range(1, num_topics + 1), negative_counts, bottom=positive_counts, color='blue', label='Negative')
plt.xlabel('Topic')
plt.ylabel('Sentiment Count')
plt.title('Sentiment Counts for Each Topic')
plt.legend()
plt.xticks(range(1, num_topics + 1))
plt.show()



